#!/bin/bash
# Generate test data for compression benchmarks
# Usage: ./scripts/generate_testdata.sh [output_dir]

set -e

# Colors
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

print_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

print_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

# Output directory
OUTPUT_DIR="${1:-testdata}"
mkdir -p "$OUTPUT_DIR"

print_info "Generating test data in: $OUTPUT_DIR"

# Function to generate data
generate_file() {
    local filename="$1"
    local size_mb="$2"
    local data_type="$3"

    local filepath="$OUTPUT_DIR/$filename"

    if [ -f "$filepath" ]; then
        print_warn "$filename already exists, skipping"
        return
    fi

    print_info "Generating $filename (${size_mb}MB, type: $data_type)..."

    case "$data_type" in
        random)
            # Random data (worst case for compression)
            dd if=/dev/urandom of="$filepath" bs=1M count=$size_mb status=progress 2>&1 | tail -1
            ;;
        zeros)
            # All zeros (best case for compression)
            dd if=/dev/zero of="$filepath" bs=1M count=$size_mb status=progress 2>&1 | tail -1
            ;;
        text)
            # Repetitive text data (good compression)
            # Generate repeated Lorem Ipsum
            local text="Lorem ipsum dolor sit amet, consectetur adipiscing elit. "
            text="${text}Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. "
            local target_size=$((size_mb * 1024 * 1024))
            > "$filepath"
            while [ $(stat -c%s "$filepath") -lt $target_size ]; do
                echo "$text" >> "$filepath"
            done
            truncate -s ${size_mb}M "$filepath"
            ;;
        binary)
            # Semi-random binary (realistic case)
            # Mix of patterns and random data
            local temp_random="${filepath}.tmp.random"
            local temp_pattern="${filepath}.tmp.pattern"

            # 70% patterned data
            dd if=/dev/zero of="$temp_pattern" bs=1M count=$((size_mb * 7 / 10)) 2>/dev/null
            # 30% random data
            dd if=/dev/urandom of="$temp_random" bs=1M count=$((size_mb * 3 / 10)) 2>/dev/null

            cat "$temp_pattern" "$temp_random" > "$filepath"
            rm -f "$temp_pattern" "$temp_random"

            truncate -s ${size_mb}M "$filepath"
            ;;
        *)
            print_warn "Unknown data type: $data_type, using random"
            dd if=/dev/urandom of="$filepath" bs=1M count=$size_mb status=progress 2>&1 | tail -1
            ;;
    esac

    local actual_size=$(stat -c%s "$filepath")
    print_info "Created $filename - $(numfmt --to=iec-i --suffix=B $actual_size)"
}

# Generate test suite
print_info "=== Generating Test Data Suite ==="
echo ""

# Small files (10MB) - quick tests
print_info "--- Small files (10MB) ---"
generate_file "small_random_10mb.bin" 10 "random"
generate_file "small_binary_10mb.bin" 10 "binary"
generate_file "small_zeros_10mb.bin" 10 "zeros"
echo ""

# Medium files (100MB) - standard tests
print_info "--- Medium files (100MB) ---"
generate_file "medium_random_100mb.bin" 100 "random"
generate_file "medium_binary_100mb.bin" 100 "binary"
generate_file "meduim_zeros_100mb.bin" 100 "zeros"
echo ""

# Large files (1GB) - stress tests
print_info "--- Large files (1GB) ---"
generate_file "large_random_1gb.bin" 1024 "random"
generate_file "large_zeros_1gb.bin" 1024 "zeros"
generate_file "large_binary_1gb.bin" 1024 "binary"
echo ""

# Extra large (4GB) - extreme stress test (optional)
read -p "Generate 4GB stress test file? (y/N) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    print_info "--- Extra large files (4GB) ---"
    generate_file "xlarge_random_4gb.bin" 4096 "random"
    generate_file "xlarge_zeros_4gb.bin" 4096 "zeros"
    generate_file "xlarge_binary_4gb.bin" 4096 "binary"
    echo ""
fi

# Summary
print_info "=== Test Data Summary ==="
ls -lh "$OUTPUT_DIR" | grep -v "^total" | awk '{printf "  %-40s %10s\n", $9, $5}'
echo ""

total_size=$(du -sh "$OUTPUT_DIR" | awk '{print $1}')
print_info "Total size: $total_size"
print_info "Location: $(realpath $OUTPUT_DIR)"

# Create a manifest
MANIFEST="$OUTPUT_DIR/manifest.txt"
cat > "$MANIFEST" << 'EOF'
# Test Data Manifest
# Generated by generate_testdata.sh

## File Descriptions

### Small Files (10MB) - Quick Testing
- small_random_10mb.bin   : Random data (incompressible)
- small_text_10mb.bin     : Repetitive text (highly compressible)
- small_zeros_10mb.bin    : All zeros (maximum compression)

### Medium Files (100MB) - Standard Benchmarks
- medium_random_100mb.bin : Random data (worst case)
- medium_binary_100mb.bin : Mixed pattern/random (realistic)
- medium_text_100mb.bin   : Text data (good compression)

### Large Files (1GB) - Stress Testing
- large_random_1gb.bin    : Random data stress test
- large_binary_1gb.bin    : Realistic data stress test

### Extra Large (4GB) - Extreme Stress (Optional)
- xlarge_random_4gb.bin   : Maximum stress test
- xlarge_zeros_4gb.bin    : Maximum stress test
- xlarge_binary_4gb.bin   : Maximum stress test

## Recommended Test Sequence

1. **Quick validation**: Use small_* files
2. **Performance testing**: Use medium_* files
3. **Stress testing**: Use large_* files
4. **Extreme cases**: Use xlarge_* files

## Compression Expectations

- random files: Low compression ratio (~1.0x)
- binary files: Medium compression (~2-4x depending on algorithm)
- text files: High compression (~5-10x)
- zeros files: Maximum compression (100x+)
EOF

print_info "Manifest created: $MANIFEST"
print_info "Test data generation complete!"
